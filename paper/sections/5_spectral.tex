\section{Spectral Structure of the Alignment Operator}
\label{sec:spectral}

The scalar diagnostic $\mathcal{A}(\theta;q)$ becomes fully transparent when
expressed through a natural mixed tensor that compares empirical and geometric
sensitivity \emph{direction by direction}. This section develops the spectral
structure of that operator and establishes the key identity
\[
\mathcal{A} = \sum_{i=1}^D (\lambda_i - 1),
\]
which provides a complete and invariant summary of empirical deformation.

% -------------------------------------------------------------
\subsection{The Alignment Operator}

Raising one index of the empirical covariance with the inverse Fisher metric
defines the $(1,1)$ alignment operator:
\begin{equation}
H^i_{\;j}(\theta;q)
    := G^{ik}(\theta)\,C_{kj}(\theta;q).
\label{eq:H_def}
\end{equation}

Using the decomposition $C_{ij} = G_{ij} + \Delta_{ij}$, we obtain
\begin{equation}
H^i_{\;j} = \delta^i_{\;j} + G^{ik}\Delta_{kj},
\label{eq:H_decomposition}
\end{equation}
so that:
\begin{itemize}
    \item $H = I$ under Fisher-equilibrium sensitivity,
    \item deviations from the identity encode empirical alignment,
    \item reinforcement and suppression manifest as eigenvalues above or below~1.
\end{itemize}

Although $H$ need not be symmetric, it inherits strong spectral structure from
the positive definiteness of the Fisher metric
\cite{AmariNagaoka2000,Amari2016}.

% -------------------------------------------------------------
\subsection{Diagonalizability and Symmetric Surrogate}

Because $G$ is positive definite, it admits a symmetric square root $G^{1/2}$
and its inverse $G^{-1/2}$ \cite{AmariNagaoka2000}. Writing
\[
H = G^{-1}C
\qquad\Longrightarrow\qquad
H \sim G^{-1/2} C\, G^{-1/2},
\]
we see that $H$ is similar to the symmetric matrix
$G^{-1/2} C\, G^{-1/2}$.

Consequences:
\begin{itemize}
    \item all eigenvalues of $H$ are real and non-negative,
    \item $H$ is diagonalizable,
    \item the spectrum of $H$ equals that of the symmetric surrogate.
\end{itemize}

Thus the empirical sensitivity deformation can be studied through a symmetric
operator without altering the underlying geometry.

% -------------------------------------------------------------
\subsection{Eigenvalues as Sensitivity Ratios}

Let $\lambda_1,\ldots,\lambda_D$ denote the eigenvalues of $H$, with eigenvectors
$u^{(i)}$ normalized with respect to the Fisher metric:
\[
G^{ij} u_i^{(k)} u_j^{(k)} = 1.
\]

Inserting these eigenvectors into the relation $Hu = \lambda u$ yields:
\begin{equation}
\lambda
    = u^{i} C_{ij} u^{j},
\label{eq:lambda_interpretation}
\end{equation}
so each eigenvalue measures the empirical variance of the score \emph{relative to}
the Fisher--Rao expectation in that direction.

Interpretation:
\begin{itemize}
    \item $\lambda_i > 1$ \quad reinforced empirical sensitivity,
    \item $\lambda_i < 1$ \quad suppressed empirical sensitivity,
    \item $\lambda_i = 1$ \quad Fisher-equivalent sensitivity.
\end{itemize}

The spectrum of $H$ therefore gives a direction-wise decomposition of empirical
alignment on the statistical manifold.

% -------------------------------------------------------------
\subsection{Trace Identity and the Scalar Diagnostic}

Taking the trace of the decomposition in
Eq.~\eqref{eq:H_decomposition} immediately gives
\[
\mathrm{Tr}(H) = D + \mathrm{Tr}(G^{-1}\Delta).
\]

Hence
\begin{equation}
\mathcal{A}(\theta;q)
    = \mathrm{Tr}(G^{-1}C) - D
    = \mathrm{Tr}(H) - D
    = \sum_{i=1}^D (\lambda_i - 1).
\label{eq:trace_identity}
\end{equation}

This establishes that $\mathcal{A}$ aggregates the total empirical deviation of
$H$ from the identity and that the rectified amplitude
\[
\phi = \max\{\sqrt{\mathcal{A}},0\}
\]
captures the \emph{total strength of excess alignment}.

% -------------------------------------------------------------
\subsection{Finite-Sample Resolution and Minimal Coherent Alignment}

The scalar diagnostic
\begin{equation}
\mathcal{A}(\theta;q) = \mathrm{Tr}(G^{-1}C) - D
\end{equation}
is defined exactly and invariantly at the population level. In empirical
settings, however, all expectations entering the empirical score covariance
$C_{ij}$ are estimated from a finite sample of size $N$. As a consequence, even
under perfect Fisher equilibrium ($q=p$), the empirical alignment operator
\begin{equation}
H = G^{-1}C
\end{equation}
does not coincide exactly with the identity, but exhibits spectral fluctuations
arising purely from finite-sample effects.

Under standard regularity assumptions and for fixed parameter dimension, the
eigenvalues $\lambda_i$ of $H$ fluctuate around unity with typical deviations of
order
\begin{equation}
\lambda_i - 1 = \mathcal{O}(N^{-1/2}),
\end{equation}
independently of any genuine geometric misalignment. Deviations at this scale
therefore do not correspond to resolvable empirical structure, but instead
reflect statistical uncertainty in the estimation of score covariances.

This observation implies the existence of a \emph{minimal geometric resolution
scale} $\epsilon_N$, below which departures from Fisher equilibrium are not
operationally meaningful. We therefore distinguish between \emph{formal
alignment}, captured by the scalar diagnostic $\mathcal{A}$, and
\emph{resolvable coherent alignment}, defined by excluding spectral deviations
that lie within the Fisher--geometric noise floor. Importantly,
$\mathcal{A}_{\min}$ is not a geometric invariant, but an operational diagnostic
whose definition depends explicitly on finite-sample resolution.

Concretely, we introduce the minimally resolvable excess-alignment diagnostic
\begin{equation}
\mathcal{A}_{\min}(\theta;q)
\;=\;
\sum_{\lambda_i > 1 + \epsilon_N} (\lambda_i - 1),
\qquad
\epsilon_N \sim \mathcal{O}(N^{-1/2}),
\end{equation}
which isolates only those eigenmodes whose empirical reinforcement exceeds the
finite-sample resolution threshold. Only such modes are interpreted as carrying
coherent geometric alignment.

This refinement does not modify the underlying construction of the alignment
operator, nor the definition of the scalar diagnostic $\mathcal{A}$ itself.
Rather, it makes explicit the empirical resolution limit inherent to finite
data. In particular, the condition $\mathcal{A}_{\min}=0$ should be interpreted
as the absence of \emph{resolvable excess alignment}, which may arise either from
true Fisher equilibrium or from empirical deviations that remain below the
finite-sample noise floor.

For empirical interpretation, it is often convenient to define a corresponding
\emph{resolvable coherent amplitude}
\begin{equation}
\phi_{\mathrm{res}}(\theta)
\;=\;
\max\{\sqrt{\mathcal{A}_{\min}(\theta;q)},\,0\},
\end{equation}
which provides a finite-sample--robust measure of the intensity of coherent
empirical reinforcement. This quantity plays an interpretive role analogous to
the formal amplitude $\phi=\max\{\sqrt{\mathcal{A}},0\}$, but suppresses
artificial inflation due to the accumulation of sub-threshold fluctuations, a
particularly relevant effect in high-dimensional models such as deep neural
networks.

Throughout this work, empirical results are therefore interpreted with respect
to both the formal diagnostic $\mathcal{A}$ and its resolvable counterparts
$\mathcal{A}_{\min}$ and $\phi_{\mathrm{res}}$. This distinction is essential in
heterogeneous or multi-scale systems, where genuine structural reinforcement may
coexist with equally strong suppression across different directions of parameter
space, yet only reinforced modes that exceed the finite-sample resolution
threshold contribute to resolvable coherent alignment.

% -------------------------------------------------------------
\subsection{Relation to Dimensionality Measures}

Although related to classical measures of effective dimensionality such as the
participation ratio and effective rank, the diagnostic $\mathcal{A}$ differs in
three key ways:
\begin{itemize}
    \item it compares empirical sensitivity directly against the Fisher baseline,
    \item it is signed, distinguishing suppression and reinforcement,
    \item it is a scalar invariant under reparametrization.
\end{itemize}

Thus $\mathcal{A}$ offers a complementary perspective to spectral summaries
commonly used in high-dimensional statistics and deep learning
\cite{Papyan2019,Sagun2016,Laurent2018,Nakkiran2020}.
