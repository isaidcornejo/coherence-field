\section{Background}
\label{sec:background}

Let $p(x|\theta)$ be a regular parametric statistical model with parameter
$\theta \in \Theta \subset \mathbb{R}^D$. The score is defined as
\[
v_i(x|\theta) = \partial_i \log p(x|\theta),
\]
and the Fisher--Rao metric is
\[
G_{ij}(\theta) = \mathbb{E}_{p}[v_i v_j]
\]
as introduced in the foundational works of Fisher and Rao
\cite{Fisher1922,Rao1945} and formalized in modern information geometry
\cite{AmariNagaoka2000,Amari2016}.

When empirical expectations are taken with respect to a distribution $q(x)$ that
differs from $p(x|\theta)$, the empirical score covariance,
\[
C_{ij}(\theta;q) = \mathbb{E}_{q}[v_i v_j],
\]
generally deviates from the Fisher metric. This mismatch produces anisotropic
empirical sensitivity.

In modern statistical models---particularly deep networks---empirical Fisher
matrices often exhibit:
\begin{itemize}
    \item heavy-tailed spectral decay,
    \item dominant outlier eigenvalues,
    \item effective dimensionality collapse,
\end{itemize}
phenomena that have been extensively documented in recent deep learning
literature \cite{Papyan2019,Sagun2016,Laurent2018,Nakkiran2020}.

These effects motivate the need for principled, invariant diagnostics capable of
characterizing empirical deviation from Fisher geometry.
